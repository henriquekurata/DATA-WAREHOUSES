**********LABORATÓRIO 4 - PIPELINE DE DADOS COM FONTE DE ARQUIVO LOCAL (FILE), MOVIMENTAÇÃO COM AIRBYTE E DESTINO NO SGBD POSTGRESQL (EXECUÇÃO EM CONTAINERS DOCKER)**********

- Ferramentas usadas nesse lab: Docker, PostgreSQL, pgAdmin, Airbyte.

- Passos: 
0.Criar imagem e container para o Airbyte;
1.Criar imagem e container para o PostgreSQL;
2.Instalar o Pgadmin
3. Acessar o Airbyte http://localhost:8000 e fazer a extração dos dados da fonte (máquina local) para o destino (SGBD)

- Comandos:
#Criar imagem e container do Airbyte no Docker
Para usuários Windows instalar o git: https://git-scm.com/download/win ou acesse: https://gitforwindows.org/

git clone https://github.com/airbytehq/airbyte.git
cd airbyte
docker-compose up

# Criar o ambiente do banco de dados local
Execute o comando abaixo no terminal ou prompt de comando para baixar a imagem e criar o container:
docker run --name dbdsa-lab4 -p 5432:5432 -e POSTGRES_USER=dsa -e POSTGRES_PASSWORD=dsa123 -e POSTGRES_DB=dsadb -d postgres

Name SGBD Pgadmin: Lab4
Schema: dsadb
Container Docker: dbdsa-lab4

#Configuração de origem e destino no Airbyte
Para a leitura de arquivos local é necessário realizar o mapeamento de volumes direto no CMD com o comando: docker cp C:\Arquivos\"Nome_Arquivo.csv" airbyte-server:\tmp\airbyte_local
Além disso, o arquivo deve estar na pasta raiz nomeada como "Arquivos"








**********LABORATÓRIO 5 -  ATUTOMAÇÃO DE PROCESSO ETL NO DW COM APACHE AIRFLOW (LINGUAGEM PYTHON) VIA DOCKER**********

- Ferramentas: 
Docker, PostgreSQL, pgAdmin, Apache Airflow e Anaconda.

- Passos: 
0.Criar imagem e container para o banco de dados do DW;
1.Criar imagem e containers para o Apache Airflow;
2.Configurar a comunicação entre as redes de containers (PostgreSQL e Airflow);
3.Criar a connetion ID no Airlow;
4.Criar a DAG;
5.Inserir a DAG dentro da pasta raiz na máquina local do Airflow;
6. Disparar a DAG.

- Comandos:
# Preparando o Container Docker Para o Banco de Dados do DW
Execute o comando abaixo no terminal ou prompt de comando para baixar a imagem do Postgres:
docker pull postgres

Execute o comando abaixo para inicializar o container:
docker run --name dbdsa -p 5433:5432 -e POSTGRES_USER=dsalabdw -e POSTGRES_PASSWORD=dsalabdw123 -e POSTGRES_DB=dwdb -d postgres

Acesse o Postgres pelo pgAdmin e crie um schema chamado dsalabdw

Name SGBD Pgadmin: Lab5
Schema: dsalabdw
Container Docker: dbdsa

# Preparando os Containers Docker para o Apache Airflow
Criar pasta vazia na raiz com o nome "Airflow" na máquina local
Navegar pelo CMD até a pasta "Airflow"

Seguir a documentação do link abaixo e executar os comandos dentro da pasta "Airflow": 
https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html

Comandos da documentação do link:
curl -LfO 'https://airflow.apache.org/docs/apache-airflow/2.4.3/docker-compose.yaml'

mkdir -p ./dags ./logs ./plugins ./config

echo -e "AIRFLOW_UID=$(id -u)" > .env

docker compose up airflow-init

docker compose up

Com a rede de container em execução, agora é só acessar a porta localhost:8080 com as credenciais abaixo:
User: airflow
Password: airflow


# Configurando a rede de containers Docker para o SGBD e o Airflow se comunicarem:
Listar as redes Docker:
docker network ls

Inspecionar o container do banco de dados:
docker inspect dbdsa

Extrair detalhes sobre a rede do container:
docker inspect dbdsa -f "{{json .NetworkSettings.Networks }}"

Inspecionar a rede de todos os containers ao mesmo tempo:
docker ps --format '{{ .ID }} {{ .Names }} {{ json .Networks }}'

Inspecionar a rede do Airflow e a rede padrão bridge:
docker network inspect airflow_default
docker network inspect bridge

Instalar ferramentas de rede no container para testar a conexão:
apt-get update
apt-get install net-tools
apt-get install iputils-ping

Fazer o teste de conexão entre o container dbdsa e o webserver do Apache Airflow:
ifconfig
ping

Para deixar tudo no mesmo ambiente de rede é necessário seguir os passos abaixo:
Desconectar o container da rede atual:
docker network disconnect bridge dbdsa

Conectar o container na rede desejada
docker network connect airflow_default dbdsa

Inspecionar a rede de todos os containers ao mesmo tempo:
docker ps --format '{{ .ID }} {{ .Names }} {{ json .Networks }}'

Extrair detalhes sobre a rede do container:
docker inspect dbdsa -f "{{json .NetworkSettings.Networks }}"

Inspecionar a rede do Airflow:
docker network inspect airflow_default

Ao acessar o Apache Airflow é necessário criar a conexão (menu > connetcion): 
Name connetion id: Lab5DW

Observações para preenchimento da connection no Airflow:
Host = usar comando ifconfig e add o INET (IP máquina do SGBD)
Schema = Nome do banco de dados
PORT = Porta do container Docker


# Job ETL (nome do arquivo: job_etl_lab5)
# Imports
import airflow
from datetime import timedelta
from airflow import DAG
from airflow.operators.postgres_operator import PostgresOperator
from airflow.utils.dates import days_ago

# Argumentos
args = {'owner': 'airflow'}

# Argumentos default
default_args = {
    'owner': 'airflow',    
    #'start_date': airflow.utils.dates.days_ago(2),
    #'end_date': datetime(),
    #'depends_on_past': False,
    #'email': ['airflow@example.com'],
    #'email_on_failure': False,
    #'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes = 5),
}

# Cria a DAG
dag_lab5_dsa = DAG(dag_id = "Lab5",
                   default_args = args,
                   # schedule_interval='0 0 * * *',
                   schedule_interval = '@once',  
                   dagrun_timeout = timedelta(minutes = 60),
                   description = 'Job ETL de Carga no DW com Airflow',
                   start_date = airflow.utils.dates.days_ago(1)
)

# Instrução SQL de criação de tabela
sql_cria_tabela = """CREATE TABLE IF NOT EXISTS tb_funcionarios (id INT NOT NULL, nome VARCHAR(250) NOT NULL, departamento VARCHAR(250) NOT NULL);"""

# Tarefa de criação da tabela
cria_tabela = PostgresOperator(sql = sql_cria_tabela,
                               task_id = "tarefa_cria_tabela",
                               postgres_conn_id = "Lab5DW",
                               dag = dag_lab5_dsa
)

# Instrução SQL de insert na tabela
sql_insere_dados = """
insert into tb_funcionarios (id, nome, departamento) values (1000, 'Bob', 'Marketing'), (1001, 'Maria', 'Contabilidade'),(1002, 'Jeremias', 'Engenharia de Dados'), (1003, 'Messi', 'Marketing') ;"""

# Tarefa de insert na tabela
insere_dados = PostgresOperator(sql = sql_insere_dados,
                                task_id = "tarefa_insere_dados",
                                postgres_conn_id = "Lab5DW",
                                dag = dag_lab5_dsa
)

# Fluxo da DAG
cria_tabela >> insere_dados

# Bloco main
if __name__ == "__main__":
    dag_lab5_dsa.cli()


Precisa levar o arquivo "job_etl_lab5" para a pasta "AIRFLOW" > "dag", ambas criadas na raiz da máquina local
Assim que o arquivo estiver na pasta raiz a DAG automaticamente irá aparecer na interface do Airflow (DAG), na porta 8080

Agora é só disparar a trigger da Dag no Airflow










***********LAB 6 - ETL COM APACHE AIRFLOW (FONTE DE DADOS COMM ARQUIVO LOCAL E DESTINO COM SGBD POSTGRESQL)**********

- Ferramentas:
Docker, PostgreSQL, pgAdmin, Apache Airflow e Anaconda.

- Passos: 
0.Criar a estrutura das tabelas direto no DW com SQL;
1.Criar a connection no Airflow (Lab6DW);
2.Criar a DAG;
3.Inserir a DAG dentro da pasta raiz na máquina local do Airflow;
4.Inserir os arquivos csv na máquina local dentro da pasta raiz (AIRFLOW > dags > dados);
5.Disparar a DAG.


Name SGBD Pgadmin: DW-Lab 6
Schema: lab6
Container docker: dbdsa (mesmo banco de dados no lab 5)

- Comandos:
# Criação de tabelas:
CREATE TABLE lab6.DIM_CLIENTE
(
    id_cliente int NOT NULL,
    nome_cliente text,
    sobrenome_cliente text,
    PRIMARY KEY (id_cliente)
);


CREATE TABLE lab6.DIM_TRANSPORTADORA
(
    id_transportadora integer NOT NULL,
    nome_transportadora text,
    PRIMARY KEY (id_transportadora)
);


CREATE TABLE lab6.DIM_DEPOSITO
(
    id_deposito bigint NOT NULL,
    nome_deposito text,
    PRIMARY KEY (id_deposito)
);


CREATE TABLE lab6.DIM_ENTREGA
(
    id_entrega bigint NOT NULL,
    endereco_entrega text,
    pais_entrega text,
    PRIMARY KEY (id_entrega)
);


CREATE TABLE lab6.DIM_PAGAMENTO
(
    id_pagamento bigint NOT NULL,
    tipo_pagamento text,
    PRIMARY KEY (id_pagamento)
);


CREATE TABLE lab6.DIM_FRETE
(
    id_frete bigint NOT NULL,
    tipo_frete text,
    PRIMARY KEY (id_frete)
);


CREATE TABLE lab6.DIM_DATA
(
    id_data bigint NOT NULL,
    data_completa text,
    dia integer,
    mes integer,
    ano integer,
    PRIMARY KEY (id_data)
);


CREATE TABLE lab6.TB_FATO
(
    id_cliente integer,
    id_transportadora integer,
    id_deposito integer,
    id_entrega integer,
    id_pagamento integer,
    id_frete integer,
    id_data integer,
    valor_entrega double precision,
    PRIMARY KEY (id_cliente, id_transportadora, id_deposito, id_entrega, id_pagamento, id_frete, id_data)
);


Obs: 
Como estamos usando Docker, se faz necessário apontar o mapeamento de volumes para os arquivos das tabelas dimensões e fato no módulo python
Ex: "op_kwargs = {'params': {'csv_file_path': '/opt/airflow/dags/dados/DIM_CLIENTE.csv'}}"
/opt/airflow/dags/dados = É a pasta criada no diretório raiz


# Job ETL - Apache Airflow (etl_dw_v5)
# Imports
import csv
import airflow
import time
import pandas as pd
from datetime import datetime
from datetime import timedelta
from airflow import DAG
from airflow.operators.python_operator import PythonOperator
from airflow.operators.postgres_operator import PostgresOperator
from airflow.utils.dates import days_ago

# Argumentos
default_args = {
    'owner': 'airflow',
    'start_date': datetime(2023, 1, 1),
    'depends_on_past': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

# Cria a DAG
# https://crontab.guru/
dag_lab6_dsa = DAG(dag_id = "lab6_final",
                   default_args = default_args,
                   schedule_interval = '0 0 * * *',
                   dagrun_timeout = timedelta(minutes = 60),
                   description = 'Job ETL de Carga no DW com Airflow',
                   start_date = airflow.utils.dates.days_ago(1)
)

##### Tabela de Clientes #####

def func_carrega_dados_clientes(**kwargs):
    
    # Get the csv file path
    csv_file_path = kwargs['params']['csv_file_path']

    # Inicializa o contador
    i = 0

    # Open the csv file
    with open(csv_file_path, 'r') as f:

        reader = csv.DictReader(f)

        for item in reader:

            # Icrementa o contador
            i += 1
            
            # Extrai uma linha como dicionário
            dados_cli = dict(item)

            # Insert data into the PostgreSQL table
            sql_query_cli = "INSERT INTO lab6.DIM_CLIENTE (%s) VALUES (%s)" % (','.join(dados_cli.keys()), ','.join([item for item in dados_cli.values()]))
    
            # Operador do Postgres com incremento no id da tarefa (para cada linha inserida)
            postgres_operator = PostgresOperator(task_id = 'carrega_dados_clientes_' + str(i), 
                                                 sql = sql_query_cli, 
                                                 params = (dados_cli), 
                                                 postgres_conn_id = 'Lab6DW', 
                                                 dag = dag_lab6_dsa)
    
            # Executa o operador
            postgres_operator.execute(context = kwargs)


tarefa_carrega_dados_clientes = PythonOperator(
        task_id = 'tarefa_carrega_dados_clientes',
        python_callable = func_carrega_dados_clientes,
        provide_context = True,
        op_kwargs = {'params': {'csv_file_path': '/opt/airflow/dags/dados/DIM_CLIENTE.csv'}},
        dag = dag_lab6_dsa
    )


##### Tabela de Transportadoras #####

def func_carrega_dados_transp(**kwargs):
    
    # Get the csv file path
    csv_file_path = kwargs['params']['csv_file_path']

    # Inicializa o contador
    i = 0

    # Open the csv file
    with open(csv_file_path, 'r') as f:

        reader = csv.DictReader(f)

        for item in reader:

            # Icrementa o contador
            i += 1
            
            # Extrai uma linha como dicionário
            dados_transp = dict(item)

            # Insert data into the PostgreSQL table
            sql_query_transp = "INSERT INTO lab6.DIM_TRANSPORTADORA (%s) VALUES (%s)" % (','.join(dados_transp.keys()), ','.join([item for item in dados_transp.values()]))
    
            # Operador do Postgres com incremento no id da tarefa (para cada linha inserida)
            postgres_operator = PostgresOperator(task_id = 'carrega_dados_transp_' + str(i), 
                                                 sql = sql_query_transp, 
                                                 params = (dados_transp), 
                                                 postgres_conn_id = 'Lab6DW', 
                                                 dag = dag_lab6_dsa)
    
            # Executa o operador
            postgres_operator.execute(context = kwargs)


tarefa_carrega_dados_transportadora = PythonOperator(
        task_id = 'tarefa_carrega_dados_transportadora',
        python_callable = func_carrega_dados_transp,
        provide_context = True,
        op_kwargs = {'params': {'csv_file_path': '/opt/airflow/dags/dados/DIM_TRANSPORTADORA.csv'}},
        dag = dag_lab6_dsa
    )

##### Tabela de Depósitos #####

def func_carrega_dados_dep(**kwargs):
    
    # Get the csv file path
    csv_file_path = kwargs['params']['csv_file_path']

    # Inicializa o contador
    i = 0

    # Open the csv file
    with open(csv_file_path, 'r') as f:

        reader = csv.DictReader(f)

        for item in reader:

            # Icrementa o contador
            i += 1
            
            # Extrai uma linha como dicionário
            dados_dep = dict(item)

            # Insert data into the PostgreSQL table
            sql_query_dep = "INSERT INTO lab6.DIM_DEPOSITO (%s) VALUES (%s)" % (','.join(dados_dep.keys()), ','.join([item for item in dados_dep.values()]))
    
            # Operador do Postgres com incremento no id da tarefa (para cada linha inserida)
            postgres_operator = PostgresOperator(task_id = 'carrega_dados_dep_' + str(i), 
                                                 sql = sql_query_dep, 
                                                 params = (dados_dep), 
                                                 postgres_conn_id = 'Lab6DW', 
                                                 dag = dag_lab6_dsa)
    
            # Executa o operador
            postgres_operator.execute(context = kwargs)


tarefa_carrega_dados_deposito = PythonOperator(
        task_id = 'tarefa_carrega_dados_deposito',
        python_callable = func_carrega_dados_dep,
        provide_context = True,
        op_kwargs = {'params': {'csv_file_path': '/opt/airflow/dags/dados/DIM_DEPOSITO.csv'}},
        dag = dag_lab6_dsa
    )

##### Tabela de Entregas #####

def func_carrega_dados_ent(**kwargs):
    
    # Get the csv file path
    csv_file_path = kwargs['params']['csv_file_path']

    # Inicializa o contador
    i = 0

    # Open the csv file
    with open(csv_file_path, 'r') as f:

        reader = csv.DictReader(f)

        for item in reader:

            # Icrementa o contador
            i += 1
            
            # Extrai uma linha como dicionário
            dados_ent = dict(item)

            # Insert data into the PostgreSQL table
            sql_query_ent = "INSERT INTO lab6.DIM_ENTREGA (%s) VALUES (%s)" % (','.join(dados_ent.keys()), ','.join([item for item in dados_ent.values()]))
    
            # Operador do Postgres com incremento no id da tarefa (para cada linha inserida)
            postgres_operator = PostgresOperator(task_id = 'carrega_dados_ent_' + str(i), 
                                                 sql = sql_query_ent, 
                                                 params = (dados_ent), 
                                                 postgres_conn_id = 'Lab6DW', 
                                                 dag = dag_lab6_dsa)
    
            # Executa o operador
            postgres_operator.execute(context = kwargs)


tarefa_carrega_dados_entrega = PythonOperator(
        task_id = 'tarefa_carrega_dados_entrega',
        python_callable = func_carrega_dados_ent,
        provide_context = True,
        op_kwargs = {'params': {'csv_file_path': '/opt/airflow/dags/dados/DIM_ENTREGA.csv'}},
        dag = dag_lab6_dsa
    )

##### Tabela de Frete #####

def func_carrega_dados_frete(**kwargs):
    
    # Get the csv file path
    csv_file_path = kwargs['params']['csv_file_path']

    # Inicializa o contador
    i = 0

    # Open the csv file
    with open(csv_file_path, 'r') as f:

        reader = csv.DictReader(f)

        for item in reader:

            # Icrementa o contador
            i += 1
            
            # Extrai uma linha como dicionário
            dados_frete = dict(item)

            # Insert data into the PostgreSQL table
            sql_query_frete = "INSERT INTO lab6.DIM_FRETE (%s) VALUES (%s)" % (','.join(dados_frete.keys()), ','.join([item for item in dados_frete.values()]))
    
            # Operador do Postgres com incremento no id da tarefa (para cada linha inserida)
            postgres_operator = PostgresOperator(task_id = 'carrega_dados_frete_' + str(i), 
                                                 sql = sql_query_frete, 
                                                 params = (dados_frete), 
                                                 postgres_conn_id = 'Lab6DW', 
                                                 dag = dag_lab6_dsa)
    
            # Executa o operador
            postgres_operator.execute(context = kwargs)


tarefa_carrega_dados_frete = PythonOperator(
        task_id = 'tarefa_carrega_dados_frete',
        python_callable = func_carrega_dados_frete,
        provide_context = True,
        op_kwargs = {'params': {'csv_file_path': '/opt/airflow/dags/dados/DIM_FRETE.csv'}},
        dag = dag_lab6_dsa
    )

##### Tabela de Tipos de Pagamentos #####

def func_carrega_dados_pagamento(**kwargs):
    
    # Get the csv file path
    csv_file_path = kwargs['params']['csv_file_path']

    # Inicializa o contador
    i = 0

    # Open the csv file
    with open(csv_file_path, 'r') as f:

        reader = csv.DictReader(f)

        for item in reader:

            # Icrementa o contador
            i += 1
            
            # Extrai uma linha como dicionário
            dados_pag = dict(item)

            # Insert data into the PostgreSQL table
            sql_query_pag = "INSERT INTO lab6.DIM_PAGAMENTO (%s) VALUES (%s)" % (','.join(dados_pag.keys()), ','.join([item for item in dados_pag.values()]))
    
            # Operador do Postgres com incremento no id da tarefa (para cada linha inserida)
            postgres_operator = PostgresOperator(task_id = 'carrega_dados_pag_' + str(i), 
                                                 sql = sql_query_pag, 
                                                 params = (dados_pag), 
                                                 postgres_conn_id = 'Lab6DW', 
                                                 dag = dag_lab6_dsa)
    
            # Executa o operador
            postgres_operator.execute(context = kwargs)


tarefa_carrega_dados_pagamento = PythonOperator(
        task_id = 'tarefa_carrega_dados_pagamento',
        python_callable = func_carrega_dados_pagamento,
        provide_context = True,
        op_kwargs = {'params': {'csv_file_path': '/opt/airflow/dags/dados/DIM_PAGAMENTO.csv'}},
        dag = dag_lab6_dsa
    )

##### Tabela de Data #####

def func_carrega_dados_data(**kwargs):
    
    # Get the csv file path
    csv_file_path = kwargs['params']['csv_file_path']

    # Inicializa o contador
    i = 0

    # Open the csv file
    with open(csv_file_path, 'r') as f:

        reader = csv.DictReader(f)

        for item in reader:

            # Icrementa o contador
            i += 1
            
            # Extrai uma linha como dicionário
            dados_data = dict(item)

            # Insert data into the PostgreSQL table
            sql_query_data = "INSERT INTO lab6.DIM_DATA (%s) VALUES (%s)" % (','.join(dados_data.keys()), ','.join([item for item in dados_data.values()]))
    
            # Operador do Postgres com incremento no id da tarefa (para cada linha inserida)
            postgres_operator = PostgresOperator(task_id = 'carrega_dados_data_' + str(i), 
                                                 sql = sql_query_data, 
                                                 params = (dados_data), 
                                                 postgres_conn_id = 'Lab6DW', 
                                                 dag = dag_lab6_dsa)
    
            # Executa o operador
            postgres_operator.execute(context = kwargs)


tarefa_carrega_dados_data = PythonOperator(
        task_id = 'tarefa_carrega_dados_data',
        python_callable = func_carrega_dados_data,
        provide_context = True,
        op_kwargs = {'params': {'csv_file_path': '/opt/airflow/dags/dados/DIM_DATA.csv'}},
        dag = dag_lab6_dsa
    )

##### Tabela de Fatos #####

def func_carrega_dados_fatos(**kwargs):
    
    # Get the csv file path
    csv_file_path = kwargs['params']['csv_file_path']

    # Inicializa o contador
    i = 0

    # Open the csv file
    with open(csv_file_path, 'r') as f:

        reader = csv.DictReader(f)

        for item in reader:

            # Icrementa o contador
            i += 1
            
            # Extrai uma linha como dicionário
            dados_fatos = dict(item)

            # Insert data into the PostgreSQL table
            sql_query_fatos = "INSERT INTO lab6.TB_FATO (%s) VALUES (%s)" % (','.join(dados_fatos.keys()), ','.join([item for item in dados_fatos.values()]))
    
            # Operador do Postgres com incremento no id da tarefa (para cada linha inserida)
            postgres_operator = PostgresOperator(task_id = 'carrega_dados_fatos_' + str(i), 
                                                 sql = sql_query_fatos, 
                                                 params = (dados_fatos), 
                                                 postgres_conn_id = 'Lab6DW', 
                                                 dag = dag_lab6_dsa)
    
            # Executa o operador
            postgres_operator.execute(context = kwargs)


tarefa_carrega_dados_fatos = PythonOperator(
        task_id = 'tarefa_carrega_dados_fatos',
        python_callable = func_carrega_dados_fatos,
        provide_context = True,
        op_kwargs = {'params': {'csv_file_path': '/opt/airflow/dags/dados/TB_FATO.csv'}},
        dag = dag_lab6_dsa
    )


# Tarefas para limpar as tabelas
tarefa_trunca_tb_fato = PostgresOperator(task_id = 'tarefa_trunca_tb_fato', postgres_conn_id = 'Lab6DW', sql = "TRUNCATE TABLE lab6.TB_FATO CASCADE", dag = dag_lab6_dsa)
tarefa_trunca_dim_cliente = PostgresOperator(task_id = 'tarefa_trunca_dim_cliente', postgres_conn_id = 'Lab6DW', sql = "TRUNCATE TABLE lab6.DIM_CLIENTE CASCADE", dag = dag_lab6_dsa)
tarefa_trunca_dim_pagamento = PostgresOperator(task_id = 'tarefa_trunca_dim_pagamento', postgres_conn_id = 'Lab6DW', sql = "TRUNCATE TABLE lab6.DIM_PAGAMENTO CASCADE", dag = dag_lab6_dsa)
tarefa_trunca_dim_frete = PostgresOperator(task_id = 'tarefa_trunca_dim_frete', postgres_conn_id = 'Lab6DW', sql = "TRUNCATE TABLE lab6.DIM_FRETE CASCADE", dag = dag_lab6_dsa)
tarefa_trunca_dim_data = PostgresOperator(task_id = 'tarefa_trunca_dim_data', postgres_conn_id = 'Lab6DW', sql = "TRUNCATE TABLE lab6.DIM_DATA CASCADE", dag = dag_lab6_dsa)
tarefa_trunca_dim_transportadora = PostgresOperator(task_id = 'tarefa_trunca_dim_transportadora', postgres_conn_id = 'Lab6DW', sql = "TRUNCATE TABLE lab6.DIM_TRANSPORTADORA CASCADE", dag = dag_lab6_dsa)
tarefa_trunca_dim_entrega = PostgresOperator(task_id = 'tarefa_trunca_dim_entrega', postgres_conn_id = 'Lab6DW', sql = "TRUNCATE TABLE lab6.DIM_ENTREGA CASCADE", dag = dag_lab6_dsa)
tarefa_trunca_dim_deposito = PostgresOperator(task_id = 'tarefa_trunca_dim_deposito', postgres_conn_id = 'Lab6DW', sql = "TRUNCATE TABLE lab6.DIM_DEPOSITO CASCADE", dag = dag_lab6_dsa)

# Upstream
tarefa_trunca_tb_fato >> tarefa_trunca_dim_cliente >> tarefa_trunca_dim_pagamento >> tarefa_trunca_dim_frete >> tarefa_trunca_dim_data >> tarefa_trunca_dim_transportadora >> tarefa_trunca_dim_entrega >> tarefa_trunca_dim_deposito >> tarefa_carrega_dados_clientes >> tarefa_carrega_dados_transportadora >> tarefa_carrega_dados_deposito >> tarefa_carrega_dados_entrega >> tarefa_carrega_dados_frete >> tarefa_carrega_dados_pagamento >> tarefa_carrega_dados_data >> tarefa_carrega_dados_fatos

# Bloco main
if __name__ == "__main__":
    dag_lab6_dsa.cli()










**********PROJETO 1 - IMPLEMENTAÇÃO DO DW E PROCESSO ETL COM AIRBYTE / LINGUAGEM SQL**********

- Ferramentas usadas nesse lab: 
Docker, PostgreSQL, pgAdmin, Airbyte, SQL.

- Passos: 
0.Criar e inserir fonte de dados no SGBD com SQL;
2.criar a conexão e o destino com Airbyte da fonte de dados (schema 1 = servidor 1) para a SA (schema 2 = servidor 2);
3.Linguegem SQL para criar a estrutura das tabelas do DW;
4.Linguagem SQL para inserir os dados no DW;
5.Utilizar o Group By para integridade dos resultados;
6.Adicionar métrica na tabela fato;
7.Criar View X Materialized View.



Obs: Para esse processo ETL vamos utilizar linguagem SQL para servidores iguais e software para servidores distintos.
O servidor de origem dos dados deve receber a menor sobrecarga possível para nao comprometer os bancos transacionais, portanto a fonte estará no servidor 1 e a staging area e destino no servidor 2.

#Servidor 1: Fonte
Name SGBD Pgadmin: dbdsafonte
Schema:schema 1
Container docker: dbdsafonte

#Servidor 2: Stating Area (Airbyte):
Name SGBD Pgadmin: dbdsadestino
Schema: schema 2
Container docker: airbyte


#Servidor 2: Destino
Name SGBD Pgadmin: dbdsadestino 
Schema: schema 3
Container docker: dbdsadestino

- Comandos:
# Container com a fonte de dados 
docker run --name dbdsafonte -p 5433:5432 -e POSTGRES_USER=dbadmin -e POSTGRES_PASSWORD=dbadmin123 -e POSTGRES_DB=postgresDB -d postgres
docker run --name dbdsadestino -p 5434:5432 -e POSTGRES_USER=dbadmin -e POSTGRES_PASSWORD=dbadmin123 -e POSTGRES_DB=postgresDB -d postgres

# Cria as tabelas da fonte diretamente no SGBD (schema 1) com SQL
CREATE TABLE schema1.ft_categorias (
    id_categoria SERIAL PRIMARY KEY,
    nome_categoria VARCHAR(255) NOT NULL
);

INSERT INTO schema1.ft_categorias (nome_categoria) VALUES ('Computadores');
INSERT INTO schema1.ft_categorias (nome_categoria) VALUES ('Smartphones');
INSERT INTO schema1.ft_categorias (nome_categoria) VALUES ('Impressoras');

CREATE TABLE schema1.ft_subcategorias (
    id_subcategoria SERIAL PRIMARY KEY,
    nome_subcategoria VARCHAR(255) NOT NULL,
    id_categoria INTEGER REFERENCES schema1.ft_categorias(id_categoria)
);

INSERT INTO schema1.ft_subcategorias (nome_subcategoria, id_categoria) VALUES ('Notebook', 1);
INSERT INTO schema1.ft_subcategorias (nome_subcategoria, id_categoria) VALUES ('Desktop', 1);
INSERT INTO schema1.ft_subcategorias (nome_subcategoria, id_categoria) VALUES ('iPhone', 2);
INSERT INTO schema1.ft_subcategorias (nome_subcategoria, id_categoria) VALUES ('Samsung Galaxy', 2);
INSERT INTO schema1.ft_subcategorias (nome_subcategoria, id_categoria) VALUES ('Laser', 3);
INSERT INTO schema1.ft_subcategorias (nome_subcategoria, id_categoria) VALUES ('Matricial', 3);

CREATE TABLE schema1.ft_produtos (
    id_produto SERIAL PRIMARY KEY,
    nome_produto VARCHAR(255) NOT NULL,
    preco_produto NUMERIC(10,2) NOT NULL,
    id_subcategoria INTEGER REFERENCES schema1.ft_subcategorias(id_subcategoria)
);

INSERT INTO schema1.ft_produtos (nome_produto, preco_produto, id_subcategoria) VALUES ('Apple MacBook Pro M2', 6589.99, 1);
INSERT INTO schema1.ft_produtos (nome_produto, preco_produto, id_subcategoria) VALUES ('Desktop Dell 16 GB', 1500.50, 1);
INSERT INTO schema1.ft_produtos (nome_produto, preco_produto, id_subcategoria) VALUES ('iPhone 14', 4140.00, 2);
INSERT INTO schema1.ft_produtos (nome_produto, preco_produto, id_subcategoria) VALUES ('Samsung Galaxy Z', 3500.99, 2);
INSERT INTO schema1.ft_produtos (nome_produto, preco_produto, id_subcategoria) VALUES ('HP 126A Original LaserJet Imaging Drum', 300.90, 3);
INSERT INTO schema1.ft_produtos (nome_produto, preco_produto, id_subcategoria) VALUES ('Epson LX-300 II USB', 350.99, 3);

CREATE TABLE schema1.ft_cidades (
    id_cidade SERIAL PRIMARY KEY,
    nome_cidade VARCHAR(255) NOT NULL
);

INSERT INTO schema1.ft_cidades (nome_cidade) VALUES
    ('Natal'),
    ('Rio de Janeiro'),
    ('Belo Horizonte'),
    ('Salvador'),
    ('Blumenau'),
    ('Curitiba'),
    ('Fortaleza'),
    ('Recife'),
    ('Porto Alegre'),
    ('Manaus');

CREATE TABLE schema1.ft_localidades (
    id_localidade SERIAL PRIMARY KEY,
    pais VARCHAR(255) NOT NULL,
    regiao VARCHAR(255) NOT NULL,
    id_cidade INTEGER REFERENCES schema1.ft_cidades(id_cidade)
);

INSERT INTO schema1.ft_localidades (pais, regiao, id_cidade) VALUES
    ('Brasil', 'Nordeste', 1),
    ('Brasil', 'Sudeste', 2),
    ('Brasil', 'Sudeste', 3),
    ('Brasil', 'Nordeste', 4),
    ('Brasil', 'Sul', 5),
    ('Brasil', 'Sul', 6),
    ('Brasil', 'Nordeste', 7),
    ('Brasil', 'Nordeste', 8),
    ('Brasil', 'Sul', 9),
    ('Brasil', 'Norte', 10);

CREATE TABLE schema1.ft_tipo_cliente (
    id_tipo SERIAL PRIMARY KEY,
    nome_tipo VARCHAR(255) NOT NULL
);

INSERT INTO schema1.ft_tipo_cliente (nome_tipo) VALUES ('Corporativo');
INSERT INTO schema1.ft_tipo_cliente (nome_tipo) VALUES ('Consumidor');
INSERT INTO schema1.ft_tipo_cliente (nome_tipo) VALUES ('Desativado');

CREATE TABLE schema1.ft_clientes (
    id_cliente SERIAL PRIMARY KEY,
    nome_cliente VARCHAR(255) NULL,
    email_cliente VARCHAR(255) NULL,
    id_cidade INTEGER REFERENCES schema1.ft_cidades(id_cidade),
    id_tipo INTEGER REFERENCES schema1.ft_tipo_cliente(id_tipo)
);

INSERT INTO schema1.ft_clientes (nome_cliente, email_cliente, id_cidade, id_tipo) VALUES ('João Silva', 'joao.silva@exemplo.com', 1, 1);
INSERT INTO schema1.ft_clientes (nome_cliente, email_cliente, id_cidade, id_tipo) VALUES ('Maria Santos', 'maria.santos@exemplo.com', 2, 2);
INSERT INTO schema1.ft_clientes (nome_cliente, email_cliente, id_cidade, id_tipo) VALUES ('Pedro Lima', 'pedro.lima@exemplo.com', 3, 2);
INSERT INTO schema1.ft_clientes (nome_cliente, email_cliente, id_cidade, id_tipo) VALUES ('Ana Rodrigues', 'ana.rodrigues@exemplo.com', 4, 2);
INSERT INTO schema1.ft_clientes (nome_cliente, email_cliente, id_cidade, id_tipo) VALUES ('José Oliveira', 'jose.oliveira@exemplo.com', 1, 2);
INSERT INTO schema1.ft_clientes (nome_cliente, email_cliente, id_cidade, id_tipo) VALUES ('Carla Santos', 'carla.santos@exemplo.com', 4, 1);
INSERT INTO schema1.ft_clientes (nome_cliente, email_cliente, id_cidade, id_tipo) VALUES ('Marcos Souza', 'marcos.souza@exemplo.com', 5, 2);
INSERT INTO schema1.ft_clientes (nome_cliente, email_cliente, id_cidade, id_tipo) VALUES ('Julia Silva', 'julia.silva@exemplo.com', 1, 1);
INSERT INTO schema1.ft_clientes (nome_cliente, email_cliente, id_cidade, id_tipo) VALUES ('Lucas Martins', 'lucas.martins@exemplo.com', 3, 3);
INSERT INTO schema1.ft_clientes (nome_cliente, email_cliente, id_cidade, id_tipo) VALUES ('Fernanda Lima', 'fernanda.lima@exemplo.com', 4, 2);


CREATE TABLE schema1.ft_vendas (
  id_transacao VARCHAR(50) NOT NULL,
  id_produto INT NOT NULL,
  id_cliente INT NOT NULL,
  id_localizacao INT NOT NULL,
  data_transacao DATE NULL,
  quantidade INT NOT NULL,
  preco_venda DECIMAL(10,2) NOT NULL,
  custo_produto DECIMAL(10,2) NOT NULL
);

-- Gerar valores aleatórios para as colunas
WITH dados AS (
  SELECT 
    floor(random() * 1000000)::text AS id_transacao,
    floor(random() * 6 + 1) AS id_produto,
    floor(random() * 10 + 1) AS id_cliente,
    floor(random() * 4 + 1) AS id_localizacao,
    '2022-01-01'::date + floor(random() * 365)::integer AS data_transacao,
    floor(random() * 10 + 1) AS quantidade,
    round(CAST(random() * 100 + 1 AS numeric), 2) AS preco_venda,
    round(CAST(random() * 50 + 1 AS numeric), 2) AS custo_produto
  FROM generate_series(1,1000)
)
-- Inserir dados na tabela
INSERT INTO schema1.ft_vendas (id_transacao, id_produto, id_cliente, id_localizacao, data_transacao, quantidade, preco_venda, custo_produto)
SELECT 
  'TRAN-' || id_transacao AS id_transacao,
  id_produto,
  id_cliente,
  id_localizacao,
  data_transacao,
  quantidade,
  round(CAST(preco_venda AS numeric),2),
  round(CAST(custo_produto AS numeric),2)
FROM dados;

# ETL Fase 1 (Fonte para a Staging Area - Schema 1 para Schema 2 - Será feito com Airbyte)
#Fazer a sincronização no Airbyte (Sync)
Configurar a font > destino > conexão > Fazer o Sync
Obs: Para a criação da conexão no Airbyte é importante utilizar as portas externas, pois o container docker do Airbyte está em uma rede diferente do container Destino.


# ETL Fase 2 (Staging Area para o DW - Schema  2 para Schema 3 - Será feito com linguagem SQL)
# Prepara os dados para a dimensão cliente
# Campos necessários: id_cliente, nome, tipo
# Query:
SELECT id_cliente, 
       nome_cliente, 
       nome_tipo
FROM schema2.st_ft_clientes tb1, schema2.st_ft_tipo_cliente tb2
WHERE tb2.id_tipo = tb1.id_tipo;



# Prepara os dados para a dimensão produto
# Campos necessários: id_produto, nome_produto, categoria, subcategoria
# Query:
SELECT id_produto, 
       nome_produto, 
       nome_categoria, 
       nome_subcategoria
FROM schema2.st_ft_produtos tb1, schema2.st_ft_subcategorias tb2, schema2.st_ft_categorias tb3
WHERE tb3.id_categoria = tb2.id_categoria
AND tb2.id_subcategoria = tb1.id_subcategoria;



# Prepara os dados para a dimensão localidade
# Campos necessários: id_localizacao, pais, regiao, estado, cidade 
# Query:
SELECT id_localidade, 
       pais, 
       regiao, 
       CASE
        WHEN nome_cidade = 'Natal' THEN 'Rio Grande do Norte'
        WHEN nome_cidade = 'Rio de Janeiro' THEN 'Rio de Janeiro'
        WHEN nome_cidade = 'Belo Horizonte' THEN 'Minas Gerais'
        WHEN nome_cidade = 'Salvador' THEN 'Bahia'
        WHEN nome_cidade = 'Blumenau' THEN 'Santa Catarina'
        WHEN nome_cidade = 'Curitiba' THEN 'Paraná'
        WHEN nome_cidade = 'Fortaleza' THEN 'Ceará'
        WHEN nome_cidade = 'Recife' THEN 'Pernambuco'
        WHEN nome_cidade = 'Porto Alegre' THEN 'Rio Grande do Sul'
        WHEN nome_cidade = 'Manaus' THEN 'Amazonas'
       END estado, 
       nome_cidade
FROM schema2.st_ft_localidades tb1, schema2.st_ft_cidades tb2
WHERE tb2.id_cidade = tb1.id_cidade;



# Prepara os dados para a dimensão tempo
# Campos necessários: id_tempo, ano, mes, dia 
# Query:
SELECT EXTRACT(YEAR FROM d)::INT, 
       EXTRACT(MONTH FROM d)::INT, 
       EXTRACT(DAY FROM d)::INT, d::DATE
FROM generate_series('2020-01-01'::DATE, '2024-12-31'::DATE, '1 day'::INTERVAL) d;



#Modelo físico (Criação de estruturas das tabelas com Surrogate Keys)
-- Tabela Dimensão Cliente
CREATE TABLE schema3.dim_cliente (
  sk_cliente SERIAL PRIMARY KEY,
  id_cliente INT NOT NULL,
  nome VARCHAR(50) NOT NULL,
  tipo VARCHAR(50) NOT NULL
);

-- Tabela Dimensão Produto
CREATE TABLE schema3.dim_produto (
  sk_produto SERIAL PRIMARY KEY,
  id_produto INT NOT NULL,
  nome_produto VARCHAR(50) NOT NULL,
  categoria VARCHAR(50) NOT NULL,
  subcategoria VARCHAR(50) NOT NULL
);

-- Tabela Dimensão Localidade
CREATE TABLE schema3.dim_localidade (
  sk_localidade SERIAL PRIMARY KEY,
  id_localidade INT NOT NULL,
  pais VARCHAR(50) NOT NULL,
  regiao VARCHAR(50) NOT NULL,
  estado VARCHAR(50) NOT NULL,
  cidade VARCHAR(50) NOT NULL
);

-- Tabela Dimensão Tempo
CREATE TABLE schema3.dim_tempo (
  sk_tempo SERIAL PRIMARY KEY,
  data_completa date,
  ano INT NOT NULL,
  mes INT NOT NULL,
  dia INT NOT NULL
);

-- Tabela Fato de Vendas
CREATE TABLE schema3.fato_vendas (
  sk_produto INT NOT NULL,
  sk_cliente INT NOT NULL,
  sk_localidade INT NOT NULL,
  sk_tempo INT NOT NULL,
  quantidade INT NOT NULL,
  preco_venda DECIMAL(10,2) NOT NULL,
  custo_produto DECIMAL(10,2) NOT NULL,
  receita_vendas DECIMAL(10,2) NOT NULL,
  PRIMARY KEY (sk_produto, sk_cliente, sk_localidade, sk_tempo),
  FOREIGN KEY (sk_produto) REFERENCES schema3.dim_produto (sk_produto),
  FOREIGN KEY (sk_cliente) REFERENCES schema3.dim_cliente (sk_cliente),
  FOREIGN KEY (sk_localidade) REFERENCES schema3.dim_localidade (sk_localidade),
  FOREIGN KEY (sk_tempo) REFERENCES schema3.dim_tempo (sk_tempo)
);



#Carregando os dados no DW
# Carrega a tabela dim_tempo

INSERT INTO schema3.dim_tempo (ano, mes, dia, data_completa)
SELECT EXTRACT(YEAR FROM d)::INT, 
       EXTRACT(MONTH FROM d)::INT, 
       EXTRACT(DAY FROM d)::INT, d::DATE
FROM generate_series('2020-01-01'::DATE, '2024-12-31'::DATE, '1 day'::INTERVAL) d;


# Carrega a tabela dim_cliente no DW a partir da Staging Area
INSERT INTO schema3.dim_cliente (id_cliente, nome, tipo)
SELECT id_cliente, 
       nome_cliente, 
       nome_tipo
FROM schema2.st_ft_clientes tb1, schema2.st_ft_tipo_cliente tb2
WHERE tb2.id_tipo = tb1.id_tipo;


# Carrega a tabela dim_produto no DW a partir da Staging Area
INSERT INTO schema3.dim_produto (id_produto, nome_produto, categoria, subcategoria)
SELECT id_produto, 
       nome_produto, 
       nome_categoria, 
       nome_subcategoria
FROM schema2.st_ft_produtos tb1, schema2.st_ft_subcategorias tb2, schema2.st_ft_categorias tb3
WHERE tb3.id_categoria = tb2.id_categoria
AND tb2.id_subcategoria = tb1.id_subcategoria;


# Carrega a tabela dim_localidade no DW a partir da Staging Area
INSERT INTO schema3.dim_localidade (id_localidade, pais, regiao, estado, cidade)
SELECT id_localidade, 
	   pais, 
	   regiao, 
	   CASE
	   	WHEN nome_cidade = 'Natal' THEN 'Rio Grande do Norte'
		WHEN nome_cidade = 'Rio de Janeiro' THEN 'Rio de Janeiro'
		WHEN nome_cidade = 'Belo Horizonte' THEN 'Minas Gerais'
		WHEN nome_cidade = 'Salvador' THEN 'Bahia'
		WHEN nome_cidade = 'Blumenau' THEN 'Santa Catarina'
		WHEN nome_cidade = 'Curitiba' THEN 'Paraná'
		WHEN nome_cidade = 'Fortaleza' THEN 'Ceará'
		WHEN nome_cidade = 'Recife' THEN 'Pernambuco'
		WHEN nome_cidade = 'Porto Alegre' THEN 'Rio Grande do Sul'
		WHEN nome_cidade = 'Manaus' THEN 'Amazonas'
	   END estado, 
	   nome_cidade
FROM schema2.st_ft_localidades tb1, schema2.st_ft_cidades tb2
WHERE tb2.id_cidade = tb1.id_cidade;


# Carga de dados na tabela Fato:
Como a origem não tem as surrogate Keys, devemos fazer joins pelo Id de cada tabela para o cálculo das métricas
INSERT INTO schema3.fato_vendas (sk_produto, 
	                              sk_cliente, 
	                              sk_localidade, 
	                              sk_tempo, 
	                              quantidade, 
	                              preco_venda, 
	                              custo_produto, 
	                              receita_vendas)
SELECT sk_produto,
	     sk_cliente,
	     sk_localidade,
       sk_tempo, 
       quantidade, 
       preco_venda, 
	     custo_produto, 
	     ROUND((CAST(quantidade AS numeric) * CAST(preco_venda AS numeric)), 2) AS receita_vendas
FROM schema2.st_ft_vendas tb1, 
     schema2.st_ft_clientes tb2, 
	   schema2.st_ft_localidades tb3, 
	   schema2.st_ft_produtos tb4,
	   schema3.dim_tempo tb5,
	   schema3.dim_produto tb6,
	   schema3.dim_localidade tb7,
	   schema3.dim_cliente tb8
WHERE tb2.id_cliente = tb1.id_cliente
AND tb3.id_localidade = tb1.id_localizacao
AND tb4.id_produto = tb1.id_produto
AND tb1.data_transacao = tb5.data_completa
AND tb2.id_cliente = tb8.id_cliente
AND tb3.id_localidade = tb7.id_localidade
AND tb4.id_produto = tb6.id_produto;



Para não haver duplicidade entre as Surogate Keys, vamos utilizar o GROUP BY na tabela Fato Vendas:
# Carrega a tabela fato_vendas
INSERT INTO schema3.fato_vendas (sk_produto, 
	                            sk_cliente, 
	                            sk_localidade, 
	                            sk_tempo, 
	                            quantidade, 
	                            preco_venda, 
	                            custo_produto, 
	                            receita_vendas)
SELECT sk_produto,
	  sk_cliente,
	  sk_localidade,
       sk_tempo, 
       SUM(quantidade) AS quantidade, 
       SUM(preco_venda) AS preco_venda, 
	  SUM(custo_produto) AS custo_produto, 
	  SUM(ROUND((CAST(quantidade AS numeric) * CAST(preco_venda AS numeric)), 2)) AS receita_vendas
FROM schema2.st_ft_vendas tb1, 
     schema2.st_ft_clientes tb2, 
	schema2.st_ft_localidades tb3, 
	schema2.st_ft_produtos tb4,
	schema3.dim_tempo tb5,
	schema3.dim_produto tb6,
	schema3.dim_localidade tb7,
	schema3.dim_cliente tb8
WHERE tb2.id_cliente = tb1.id_cliente
AND tb3.id_localidade = tb1.id_localizacao
AND tb4.id_produto = tb1.id_produto
AND tb1.data_transacao = tb5.data_completa
AND tb2.id_cliente = tb8.id_cliente
AND tb3.id_localidade = tb7.id_localidade
AND tb4.id_produto = tb6.id_produto
GROUP BY sk_produto, sk_cliente, sk_localidade, sk_tempo;


#Adicionando uma nova métrica na tabela Fato:
# Limpa a tabela
TRUNCATE TABLE schema3.fato_vendas;

# Adiciona a nova coluna
ALTER TABLE IF EXISTS schema3.fato_vendas
    ADD COLUMN resultado numeric(10, 2) NOT NULL;

# Carrega a tabela fato
INSERT INTO schema3.fato_vendas (sk_produto, 
                                sk_cliente, 
                                sk_localidade, 
                                sk_tempo, 
                                quantidade, 
                                preco_venda, 
                                custo_produto, 
                                receita_vendas,
                                resultado)
SELECT sk_produto,
       sk_cliente,
       sk_localidade,
       sk_tempo, 
       SUM(quantidade) AS quantidade, 
       SUM(preco_venda) AS preco_venda, 
       SUM(custo_produto) AS custo_produto, 
       SUM(ROUND((CAST(quantidade AS numeric) * CAST(preco_venda AS numeric)), 2)) AS receita_vendas,
       SUM(ROUND((CAST(quantidade AS numeric) * CAST(preco_venda AS numeric)), 2) - custo_produto) AS resultado 
FROM schema2.st_ft_vendas tb1, 
     schema2.st_ft_clientes tb2, 
     schema2.st_ft_localidades tb3, 
     schema2.st_ft_produtos tb4,
     schema3.dim_tempo tb5,
     schema3.dim_produto tb6,
     schema3.dim_localidade tb7,
     schema3.dim_cliente tb8
WHERE tb2.id_cliente = tb1.id_cliente
AND tb3.id_localidade = tb1.id_localizacao
AND tb4.id_produto = tb1.id_produto
AND to_char(tb1.data_transacao, 'YYYY-MM-DD') = to_char(tb5.data_completa, 'YYYY-MM-DD')
AND to_char(tb1.data_transacao, 'HH') = tb5.hora
AND tb2.id_cliente = tb8.id_cliente
AND tb3.id_localidade = tb7.id_localidade
AND tb4.id_produto = tb6.id_produto
GROUP BY sk_produto, sk_cliente, sk_localidade, sk_tempo;


#Usando Materialized View para melhorar a performance das consultas:
# View e View Materializada

# Cria uma view (Grava a Query, porém o plano de execução ainda é realizado)
CREATE VIEW schema3.vw_relatorio
AS
SELECT estado, 
       categoria, 
       tipo AS tipo_cliente, 
       hora, 
       SUM(resultado)
FROM schema3.dim_produto AS tb1, 
     schema3.dim_cliente AS tb2, 
     schema3.dim_localidade AS tb3, 
     schema3.dim_tempo AS tb4, 
     schema3.fato_vendas AS tb5
WHERE tb5.sk_produto = tb1.sk_produto
AND tb5.sk_cliente = tb2.sk_cliente
AND tb5.sk_localidade = tb3.sk_localidade
AND tb5.sk_tempo = tb4.sk_tempo 
GROUP BY estado, categoria, tipo, hora
ORDER BY estado, categoria, tipo, hora;


SELECT * FROM schema3.vw_relatorio


# Cria uma view materializada (Com a MV não há execução de query, portanto a performance melhora)
CREATE MATERIALIZED VIEW schema3.mv_relatorio AS
SELECT estado, 
       categoria, 
       tipo AS tipo_cliente, 
       hora, 
       SUM(resultado)
FROM schema3.dim_produto AS tb1, 
     schema3.dim_cliente AS tb2, 
     schema3.dim_localidade AS tb3, 
     schema3.dim_tempo AS tb4, 
     schema3.fato_vendas AS tb5
WHERE tb5.sk_produto = tb1.sk_produto
AND tb5.sk_cliente = tb2.sk_cliente
AND tb5.sk_localidade = tb3.sk_localidade
AND tb5.sk_tempo = tb4.sk_tempo 
GROUP BY estado, categoria, tipo, hora
ORDER BY estado, categoria, tipo, hora;

SELECT * FROM schema3.mv_relatorio;






**********PROJETO 2 - CLOUD COMPUTING DATA WAREHOUSE COM TERRAFORM NA AWS (REDSHIFT)**********

- Ferramentas:
Docker, Terraform, AWS Redshift, AWS CLI.

- Passos: 
1.Acessar conta AWS e criar as credenciais de segurança;
2.Criar container docker para máquina cliente;
3.Instalar AWS CLI e Terraform no container;
4.Criar o arquivo Terraform no container;
5.Executar o terraform, aplicar infraestrutura e destrui (init, apply e destroy).

- Comandos:

#A conexão entre o Terraform e o Redshift será feito pelo AWS Cli, para isso funcionar será necessário criar as credenciais de segurança para aceso remoto (criar diretamente no console da AWS)]

# Preparação da Máquina Cliente Para o Projeto 2
# Cria um container Docker (na sua máquina local)
docker run -dti --name dsa_projeto2 --rm ubuntu 

# Instala utilitários
apt-get update
apt-get upgrade
apt-get install curl nano wget unzip

# Cria pasta de Downloads
mkdir Downloads
cd Downloads

# Download do AWS CLI
curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"

# Unzip e install
unzip awscliv2.zip
./aws/install

# Versão
aws --version

# Configura AWS CLI
aws configure

Access key ID: coloque a sua chave
Secret access key: coloque a sua chave
Default region name: us-east-2
Default output format: deixe em branco e pressione enter

# Teste
aws s3 ls

# Instala o Terraform
apt-get update && apt-get install -y gnupg software-properties-common

wget -O- https://apt.releases.hashicorp.com/gpg | gpg --dearmor | tee /usr/share/keyrings/hashicorp-archive-keyring.gpg

echo "deb [signed-by=/usr/share/keyrings/hashicorp-archive-keyring.gpg] https://apt.releases.hashicorp.com $(lsb_release -cs) main" | tee /etc/apt/sources.list.d/hashicorp.list

apt update

apt-get install terraform

# Versão do Terraform
terraform -version

Criar pasta no container Docker com o nome do projeto que está o arquivo main.tf:
Nesse caso: Container dsa_projeto2 > mkdir terraform-aws-hcl na pasta raiz do container (~)

#Arquivo main.tf
Criar o arquivo inserindo os dados abaixo com o editor de texto:
provider "aws" {
  region = "us-east-2"
}

resource "aws_security_group" "allow_http_ssh" {
  name        = "allow_http_ssh"
  description = "Allow HTTP and SSH traffic"

  ingress {
    from_port   = 22
    to_port     = 22
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  ingress {
    from_port   = 80
    to_port     = 80
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }
}

resource "aws_instance" "web_server" {
  ami           = "ami-0c55b159cbfafe1f0" # Amazon Linux 2 AMI ID
  instance_type = "dc2.large"

  vpc_security_group_ids = [aws_security_group.allow_http_ssh.id]

  tags = {
    Name = "Web Server"
  }
}


#Executar o terraform, aplicar infraestrutura e destruir 
terraform init (Inicialização do Teraform)
terraform apply (Validação para já executar o script)
terraform destroy (Limpa tudo - Grupo de segurança e instância EC2)


#Arquivo main.tf
#Preparando cluster Redshift para o DW usando infraestrutura como códico com Terraform
# Configura o Provedor AWS
provider "aws" {
  region = "us-east-2"
}


# Configura a Redshift VPC (Organização lógica com range de endereços IP)
resource "aws_vpc" "redshift_vpc" {
  cidr_block = "10.0.0.0/16"

  tags = {
    Name = "Redshift VPC"
  }
}

# Configura a Redshift Subnet (Divisão da VPC para conctar os serviços a infraestrutura, exemplo: DW em uma subnet e aplicação ETL em outra subnet, para dar maior segurança)
resource "aws_subnet" "redshift_subnet" {
  cidr_block = "10.0.1.0/24"
  vpc_id     = aws_vpc.redshift_vpc.id

  tags = {
    Name = "Redshift Subnet"
  }
}

# Configura um Gateway da Internet e Anexa a VPC (Como endereços 10/172/192 são apenas internos, se faz necessário a criação da intenet gateway abrindo assim para a internet externa)
resource "aws_internet_gateway" "redshift_igw" {
  vpc_id = aws_vpc.redshift_vpc.id

  tags = {
    Name = "Redshift Internet Gateway"
  }
}

# Configura Uma Tabela de Roteamento (Configuração para a rota de saída com a internet)
resource "aws_route_table" "redshift_route_table" {
  vpc_id = aws_vpc.redshift_vpc.id

  route {
    cidr_block = "0.0.0.0/0"
    gateway_id = aws_internet_gateway.redshift_igw.id
  }

  tags = {
    Name = "Redshift Route Table"
  }
}

# Associa a Tabela de Roteamento à Subnet (Configura a saída para as Subnets)
resource "aws_route_table_association" "redshift_route_table_association" {
  subnet_id      = aws_subnet.redshift_subnet.id
  route_table_id = aws_route_table.redshift_route_table.id
}

# Configura Um Grupo de Segurança de Acesso ao Data Warehouse com Redshift
resource "aws_security_group" "redshift_sg" {
  name        = "redshift_sg"
  description = "Allow Redshift traffic"
  vpc_id      = aws_vpc.redshift_vpc.id

  ingress {
    from_port   = 5439
    to_port     = 5439
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  tags = {
    Name = "Redshift Security Group"
  }
}

# Configura Um Grupo de Subnets Redshift
resource "aws_redshift_subnet_group" "redshift_subnet_group" {
  name       = "redshift-subnet-group"
  subnet_ids = [aws_subnet.redshift_subnet.id]

  tags = {
    Name = "Redshift Subnet Group"
  }
}

# Configura Um Cluster Redshift (Será feito com apenas uma máquina)
resource "aws_redshift_cluster" "redshift_cluster" {
  cluster_identifier = "redshift-cluster"
  database_name      = "dsadb"
  master_username    = "adminuser"
  master_password    = "dsaSecurePassw0rd!"
  node_type          = "dc2.large"
  number_of_nodes    = 1

  vpc_security_group_ids = [aws_security_group.redshift_sg.id]
  cluster_subnet_group_name = aws_redshift_subnet_group.redshift_subnet_group.name

  skip_final_snapshot = true
}


Acessando o Redshift: Redshift-cluster > Query data > Conectar ao banco de dados com nome e senha do arquivo main.tf







# Deploy do DW na AWS com Terraform

- Ferramentas:
Docker, Terraform, AWS Redshift, AWS CLI, SQL.

- Passos: 
Já estão listados junto com os comandos.

- Comandos:

1- Acesse sua conta AWS e crie um bucket na região de Ohio.

2- Dentro do bucket crie uma pasta chamada dados.

3- Faça o upload dos 5 arquivos CSV para essa pasta criada.

4-Criar o container Docker local.
# Preparação da Máquina Cliente Para o Projeto 2
# Cria um container Docker (na sua máquina local) com PostgreSQL e bibliotecas de conexão cliente
docker run --name cliente_dsa -p 5438:5432 -e POSTGRES_USER=dsadmin -e POSTGRES_PASSWORD=dsadmin123 -e POSTGRES_DB=dsdb -d postgres

# Instala utilitários
apt-get update
apt-get upgrade
apt-get install curl nano wget unzip vim sudo

# Cria pasta de Downloads
mkdir Downloads
cd Downloads

# Download do AWS CLI
curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"

# Unzip e install
unzip awscliv2.zip
./aws/install

# Versão
aws --version

# Configura AWS CLI
aws configure

Access key ID: coloque a sua chave
Secret access key: coloque a sua chave
Default region name: us-east-2
Default output format: deixe em branco e pressione enter

# Teste
aws s3 ls

# Instala o Terraform
apt-get update && apt-get install -y gnupg software-properties-common

wget -O- https://apt.releases.hashicorp.com/gpg | gpg --dearmor | tee /usr/share/keyrings/hashicorp-archive-keyring.gpg

echo "deb [signed-by=/usr/share/keyrings/hashicorp-archive-keyring.gpg] https://apt.releases.hashicorp.com $(lsb_release -cs) main" | tee /etc/apt/sources.list.d/hashicorp.list

apt update

apt-get install terraform

# Versão do Terraform
terraform -version

5- Acesse o terminal do container e crie as pastas abaixo:
cd ~
mkdir projeto2
cd projeto2
mkdir etapa1
cd etapa1

6- Na pasta etapa1 crie os arquivos abaixo:
touch provider.tf
touch redshift.tf
touch redshift_role.tf

7- Edite cada um dos arquivos com o mesmo conteúdo dos arquivos fornecidos a você (acompanhe as aulas em vídeo sempre com o máximo de atenção).
nano provider.tf
nano redshift.tf
nano redshift_role.tf

provider:
provider "aws" {
  region = "us-east-2"
}

redshift.tf:
# Configura a Redshift VPC
resource "aws_vpc" "redshift_vpc" {
  cidr_block = "10.0.0.0/16"

  tags = {
    Name = "Redshift VPC"
  }
}

# Configura a Redshift Subnet
resource "aws_subnet" "redshift_subnet" {
  cidr_block = "10.0.1.0/24"
  vpc_id     = aws_vpc.redshift_vpc.id

  tags = {
    Name = "Redshift Subnet"
  }
}

# Configura um Gateway da Internet e Anexa a VPC
resource "aws_internet_gateway" "redshift_igw" {
  vpc_id = aws_vpc.redshift_vpc.id

  tags = {
    Name = "Redshift Internet Gateway"
  }
}

# Configura Uma Tabela de Roteamento
resource "aws_route_table" "redshift_route_table" {
  vpc_id = aws_vpc.redshift_vpc.id

  route {
    cidr_block = "0.0.0.0/0"
    gateway_id = aws_internet_gateway.redshift_igw.id
  }

  tags = {
    Name = "Redshift Route Table"
  }
}

# Associa a Tabela de Roteamento à Subnet
resource "aws_route_table_association" "redshift_route_table_association" {
  subnet_id      = aws_subnet.redshift_subnet.id
  route_table_id = aws_route_table.redshift_route_table.id
}

# Configura Um Grupo de Segurança de Acesso ao Data Warehouse com Redshift
resource "aws_security_group" "redshift_sg" {
  name        = "redshift_sg"
  description = "Allow Redshift traffic"
  vpc_id      = aws_vpc.redshift_vpc.id

  ingress {
    from_port   = 5439
    to_port     = 5439
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  tags = {
    Name = "Redshift Security Group"
  }
}

# Configura Um Grupo de Subnets Redshift
resource "aws_redshift_subnet_group" "redshift_subnet_group" {
  name       = "redshift-subnet-group"
  subnet_ids = [aws_subnet.redshift_subnet.id]

  tags = {
    Name = "Redshift Subnet Group"
  }
}

# Configura Um Cluster Redshift 
resource "aws_redshift_cluster" "redshift_cluster" {
  cluster_identifier = "redshift-cluster"
  database_name      = "dsadb"
  master_username    = "adminuser"
  master_password    = "dsaS9curePassw2rd"
  node_type          = "dc2.large"
  number_of_nodes    = 1

  vpc_security_group_ids = [aws_security_group.redshift_sg.id]
  cluster_subnet_group_name = aws_redshift_subnet_group.redshift_subnet_group.name
  iam_roles = [aws_iam_role.redshift_role.arn]

  skip_final_snapshot = true
}



redshift_role: (IAM é o privilégio de acesso entre serviços distintos da AWS)
resource "aws_iam_role" "redshift_role" {
  name = "RedshiftS3AccessRole"

  assume_role_policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Action = "sts:AssumeRole"
        Effect = "Allow"
        Principal = {
          Service = "redshift.amazonaws.com"
        }
      }
    ]
  })
}

resource "aws_iam_role_policy_attachment" "redshift_s3_read" {
  policy_arn = "arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess"
  role       = aws_iam_role.redshift_role.name
}



8- Pelo terminal, na pasta etapa1, execute os comandos abaixo:
terraform init
terraform validate
terraform plan
terraform apply

9- Acesse o painel do Redshift na AWS e confirme que o cluster do Redshift foi criado para o DW.

10- Acesse o painel do IAM na AWS e verifique se a role RedshiftS3AccessRole foi criada. Copie o endereço ARN da role e coloque no arquivo load_data.sql.

11- Crie a pasta etapa2 no container:
cd ~
cd projeto2
mkdir etapa2
cd etapa2

12- Dentro da pasta etapa2 coloque o arquivo load_data.sql (fornecido a você). Lembre-se de alterar o ACCOUNT ID para o número da sua conta AWS.

touch load_data.sql
nano load_data.sql

Arquivo load_data.sql:
CREATE SCHEMA IF NOT EXISTS dsaschema;

CREATE TABLE IF NOT EXISTS dsaschema.dim_cliente 
(
    sk_cliente integer NOT NULL,
    id_cliente integer NOT NULL,
    nome character varying(50) NOT NULL,
    tipo character varying(50),
    CONSTRAINT dim_cliente_pkey PRIMARY KEY (sk_cliente)
);

CREATE TABLE IF NOT EXISTS dsaschema.dim_localidade
(
    sk_localidade integer NOT NULL,
    id_localidade integer NOT NULL,
    pais character varying(50) NOT NULL,
    regiao character varying(50) NOT NULL,
    estado character varying(50) NOT NULL,
    cidade character varying(50) NOT NULL,
    CONSTRAINT dim_localidade_pkey PRIMARY KEY (sk_localidade)
);

CREATE TABLE IF NOT EXISTS dsaschema.dim_produto
(
    sk_produto integer NOT NULL,
    id_produto integer NOT NULL,
    nome_produto character varying(50) NOT NULL,
    categoria character varying(50) NOT NULL,
    subcategoria character varying(50) NOT NULL,
    CONSTRAINT dim_produto_pkey PRIMARY KEY (sk_produto)
);

CREATE TABLE IF NOT EXISTS dsaschema.dim_tempo
(
    sk_tempo integer NOT NULL,
    data_completa date,
    ano integer NOT NULL,
    mes integer NOT NULL,
    dia integer NOT NULL,
    CONSTRAINT dim_tempo_pkey PRIMARY KEY (sk_tempo)
);

CREATE TABLE IF NOT EXISTS dsaschema.fato_vendas
(
    sk_produto integer NOT NULL,
    sk_cliente integer NOT NULL,
    sk_localidade integer NOT NULL,
    sk_tempo integer NOT NULL,
    quantidade integer NOT NULL,
    preco_venda numeric(10,2) NOT NULL,
    custo_produto numeric(10,2) NOT NULL,
    receita_vendas numeric(10,2) NOT NULL,
    CONSTRAINT fato_vendas_pkey PRIMARY KEY (sk_produto, sk_cliente, sk_localidade, sk_tempo),
    CONSTRAINT fato_vendas_sk_cliente_fkey FOREIGN KEY (sk_cliente) REFERENCES dsaschema.dim_cliente (sk_cliente),
    CONSTRAINT fato_vendas_sk_localidade_fkey FOREIGN KEY (sk_localidade) REFERENCES dsaschema.dim_localidade (sk_localidade),
    CONSTRAINT fato_vendas_sk_produto_fkey FOREIGN KEY (sk_produto) REFERENCES dsaschema.dim_produto (sk_produto),
    CONSTRAINT fato_vendas_sk_tempo_fkey FOREIGN KEY (sk_tempo) REFERENCES dsaschema.dim_tempo (sk_tempo)
);

COPY dsaschema.dim_cliente
FROM 's3://dsa-projeto2/dados/dim_cliente.csv'
IAM_ROLE 'arn:aws:iam::890582101704:role/RedshiftS3AccessRole'
CSV;

COPY dsaschema.dim_localidade
FROM 's3://dsa-projeto2/dados/dim_localidade.csv'
IAM_ROLE 'arn:aws:iam::890582101704:role/RedshiftS3AccessRole'
CSV;

COPY dsaschema.dim_produto
FROM 's3://dsa-projeto2/dados/dim_produto.csv'
IAM_ROLE 'arn:aws:iam::890582101704:role/RedshiftS3AccessRole'
CSV;

COPY dsaschema.dim_tempo
FROM 's3://dsa-projeto2/dados/dim_tempo.csv'
IAM_ROLE 'arn:aws:iam::890582101704:role/RedshiftS3AccessRole'
CSV;

COPY dsaschema.fato_vendas
FROM 's3://dsa-projeto2/dados/fato_vendas.csv'
IAM_ROLE 'arn:aws:iam::890582101704:role/RedshiftS3AccessRole'
CSV;


13- Copie o endpoint do seu cluster Redshift e ajuste o comando abaixo e então execute no terminal do container dentro da pasta etapa2. Digite a senha (dsaS9curePassw2rd) quando solicitado.
psql -h redshift-cluster.cbwssuxzxipm.us-east-2.redshift.amazonaws.com -U adminuser -d dsadb -p 5439 -f load_data.sql
O comando acima irá criar e inserir o schema e os dados no banco Redshift

14- Edite o arquivo redshift.tf e acrescente a linha abaixo como mostrado nas aulas em vídeo para associar a role do S3 ao cluster Redshift.

iam_roles = [aws_iam_role.redshift_role.arn]

15- Execute novamente o terraform apply para modificar o cluster em tempo real. Repita o passo 13. Seu DW está pronto para uso.

16- Acesse o editor de consultas do Redshift e confira se os dados foram carregados.

20- Quando terminar o trabalho, destrua a infra com o comando: terraform destroy.







